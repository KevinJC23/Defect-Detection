{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4212f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.retinanet import (\n",
    "    RetinaNetClassificationHead,\n",
    "    RetinaNetRegressionHead\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089d8f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ Training on GPU #0: NVIDIA GeForce RTX 5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ 1) Reproducibility & GPU selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "available = GPUtil.getAvailable(order='memory', limit=1)\n",
    "if available and torch.cuda.is_available():\n",
    "    gpu_id = available[0]\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    print(f\"ðŸŽ Training on GPU #{gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸  No GPU available, using CPU instead\")\n",
    "\n",
    "# â”€â”€â”€ 2) Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Expecting:\n",
    "# project_dir/\n",
    "# â”œâ”€â”€ CANS/\n",
    "# â”‚   â”œâ”€â”€ defect/\n",
    "# â”‚   â””â”€â”€ non/\n",
    "# â””â”€â”€ this_script.py (or RetinaNet.ipynb)\n",
    "\n",
    "ROOT    = os.path.join(os.getcwd(), \"CANS\")\n",
    "DEF_DIR = os.path.join(ROOT, \"defect\")\n",
    "NON_DIR = os.path.join(ROOT, \"non\")\n",
    "AUG_DIR = os.path.join(ROOT, \"augmented\")\n",
    "IMG_OUT = os.path.join(AUG_DIR, \"images\")\n",
    "ANN_OUT = os.path.join(AUG_DIR, \"annotations\")\n",
    "\n",
    "for d in (IMG_OUT, ANN_OUT):\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08e277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 3) Helper: write Pascal-VOC XML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def write_voc_xml(fn, size, boxes, labels, dest):\n",
    "    ann = ET.Element(\"annotation\")\n",
    "    ET.SubElement(ann, \"filename\").text = fn\n",
    "    size_el = ET.SubElement(ann, \"size\")\n",
    "    ET.SubElement(size_el, \"width\").text  = str(size[0])\n",
    "    ET.SubElement(size_el, \"height\").text = str(size[1])\n",
    "    ET.SubElement(size_el, \"depth\").text  = \"3\"\n",
    "    for box, lbl in zip(boxes, labels):\n",
    "        obj = ET.SubElement(ann, \"object\")\n",
    "        ET.SubElement(obj, \"name\").text = lbl\n",
    "        bb = ET.SubElement(obj, \"bndbox\")\n",
    "        ET.SubElement(bb, \"xmin\").text = str(box[0])\n",
    "        ET.SubElement(bb, \"ymin\").text = str(box[1])\n",
    "        ET.SubElement(bb, \"xmax\").text = str(box[2])\n",
    "        ET.SubElement(bb, \"ymax\").text = str(box[3])\n",
    "    tree = ET.ElementTree(ann)\n",
    "    tree.write(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a441cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€â”€ 4) Preprocess & Augment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess_and_augment(src_dir, label, num_aug=3, size=(300,300)):\n",
    "    for imgfn in os.listdir(src_dir):\n",
    "        if not imgfn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        im = Image.open(os.path.join(src_dir, imgfn)).convert(\"RGB\")\n",
    "        im = im.resize(size)\n",
    "        base, ext = os.path.splitext(imgfn)\n",
    "\n",
    "        # Original\n",
    "        out_name = f\"{label}_{base}{ext}\"\n",
    "        im.save(os.path.join(IMG_OUT, out_name))\n",
    "        xml_name = out_name.replace(ext, \".xml\")\n",
    "        write_voc_xml(\n",
    "            fn     = out_name,\n",
    "            size   = size,\n",
    "            boxes  = [[0, 0, size[0], size[1]]],\n",
    "            labels = [label],\n",
    "            dest   = os.path.join(ANN_OUT, xml_name)\n",
    "        )\n",
    "\n",
    "        # Horizontal flips\n",
    "        for i in range(num_aug):\n",
    "            aug = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            aug_name = f\"{label}_{base}_flip{i}{ext}\"\n",
    "            aug.save(os.path.join(IMG_OUT, aug_name))\n",
    "            xml_name = aug_name.replace(ext, \".xml\")\n",
    "            write_voc_xml(\n",
    "                fn     = aug_name,\n",
    "                size   = size,\n",
    "                boxes  = [[0, 0, size[0], size[1]]],\n",
    "                labels = [label],\n",
    "                dest   = os.path.join(ANN_OUT, xml_name)\n",
    "            )\n",
    "\n",
    "# Run preprocessing\n",
    "preprocess_and_augment(DEF_DIR, \"defect\",     num_aug=3)\n",
    "preprocess_and_augment(NON_DIR, \"non-defect\", num_aug=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170cb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 5) Dataset & DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, tf=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.tf      = tf\n",
    "        self.files   = [f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.files[idx]\n",
    "        im = Image.open(os.path.join(self.img_dir, fn)).convert(\"RGB\")\n",
    "        if self.tf:\n",
    "            im = self.tf(im)\n",
    "\n",
    "        tree = ET.parse(os.path.join(self.ann_dir, fn.replace(\".jpg\",\".xml\")))\n",
    "        root = tree.getroot()\n",
    "        boxes, labels = [], []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            b = obj.find(\"bndbox\")\n",
    "            boxes.append([\n",
    "                int(b.find(\"xmin\").text),\n",
    "                int(b.find(\"ymin\").text),\n",
    "                int(b.find(\"xmax\").text),\n",
    "                int(b.find(\"ymax\").text),\n",
    "            ])\n",
    "            lbl = obj.find(\"name\").text\n",
    "            labels.append(0 if lbl==\"defect\" else 1)\n",
    "\n",
    "        return (\n",
    "            im.to(device),\n",
    "            {\n",
    "                \"boxes\":  torch.tensor(boxes, dtype=torch.float32, device=device),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64, device=device)\n",
    "            }\n",
    "        )\n",
    "\n",
    "tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset    = VOCDataset(IMG_OUT, ANN_OUT, tf)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a69e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters are on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€ 6) Model Initialization & Head Replacement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "weights = RetinaNet_ResNet50_FPN_Weights.COCO_V1\n",
    "model = retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "in_feats    = model.backbone.out_channels\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_feats,\n",
    "    num_anchors=num_anchors,\n",
    "    num_classes=3  # defect, non-defect, background\n",
    ")\n",
    "model.head.regression_head = RetinaNetRegressionHead(\n",
    "    in_channels=in_feats,\n",
    "    num_anchors=num_anchors\n",
    ")\n",
    "\n",
    "model.transform.min_size = (300,)\n",
    "model.transform.max_size = 300\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model parameters are on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7bf54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 7) Optimizer & Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "optimizer    = optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc824b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 8) mAP@0.5 Utility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def calc_map(preds, targs, thr=0.5):\n",
    "    p_b, p_s, p_l = preds\n",
    "    t_b, t_l      = targs\n",
    "    precisions = []\n",
    "    for pb, ps, pl, tb, tl in zip(p_b, p_s, p_l, t_b, t_l):\n",
    "        if pb.numel()==0 or tb.numel()==0:\n",
    "            continue\n",
    "        ious = box_iou(pb, tb)\n",
    "        tp   = (ious.max(1)[0] > thr).sum().item()\n",
    "        fp   = pb.size(0) - tp\n",
    "        precisions.append(tp / (tp + fp + 1e-8))\n",
    "    return sum(precisions)/len(precisions) if precisions else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:38<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 1: Loss=0.3977, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:09<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 2: Loss=0.1297, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:07<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 3: Loss=0.0256, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:14<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 4: Loss=0.0176, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:06<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 5: Loss=0.0138, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:07<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 6: Loss=0.0064, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:42<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 7: Loss=0.0059, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [01:43<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Epoch 8: Loss=0.0056, mAP@0.5=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 562/860 [01:10<00:40,  7.37it/s]"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ 9) Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_EPOCHS = 15\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_pb, all_ps, all_pl = [], [], []\n",
    "    all_tb, all_tl         = [], []\n",
    "\n",
    "    loop = tqdm(data_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for imgs, targets in loop:\n",
    "        # ensure everything is on the correct device\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        for t in targets:\n",
    "            t[\"boxes\"]  = t[\"boxes\"].to(device)\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        # forward & backward\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # batch-level eval\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outs = model(imgs)\n",
    "            for out, gt in zip(outs, targets):\n",
    "                keep = out[\"scores\"] > 0.3\n",
    "                all_pb.append(   out[\"boxes\"][keep].cpu()   )\n",
    "                all_ps.append(  out[\"scores\"][keep].cpu()   )\n",
    "                all_pl.append( out[\"labels\"][keep].cpu()   )\n",
    "                all_tb.append(   gt[\"boxes\"].cpu()           )\n",
    "                all_tl.append(  gt[\"labels\"].cpu()          )\n",
    "        model.train()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    mAP50    = calc_map((all_pb, all_ps, all_pl), (all_tb, all_tl)) * 100\n",
    "    print(f\"â†’ Epoch {epoch}: Loss={avg_loss:.4f}, mAP@0.5={mAP50:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 10) Save Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_path = os.path.join(os.getcwd(), \"retinanet_cans.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"Model weights saved to:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
