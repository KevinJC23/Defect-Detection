{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59543dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822b4ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèé Training on GPU #0: NVIDIA GeForce RTX 5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1) Reproducibility & GPU selection\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "available = GPUtil.getAvailable(order='memory', limit=1)\n",
    "if available and torch.cuda.is_available():\n",
    "    gpu_id = available[0]\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    print(f\"üèé Training on GPU #{gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è  No GPU available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85bf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Paths\n",
    "ROOT    = os.path.join(os.getcwd(), \"CANS\")\n",
    "DEF_DIR = os.path.join(ROOT, \"defect\")\n",
    "NON_DIR = os.path.join(ROOT, \"non\")\n",
    "AUG_DIR = os.path.join(ROOT, \"augmented\")\n",
    "IMG_OUT = os.path.join(AUG_DIR, \"images\")\n",
    "ANN_OUT = os.path.join(AUG_DIR, \"annotations\")\n",
    "for d in (IMG_OUT, ANN_OUT):\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20dc89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Helper: write Pascal-VOC XML\n",
    "def write_voc_xml(fn, size, boxes, labels, dest):\n",
    "    ann = ET.Element(\"annotation\")\n",
    "    ET.SubElement(ann, \"filename\").text = fn\n",
    "    size_el = ET.SubElement(ann, \"size\")\n",
    "    ET.SubElement(size_el, \"width\").text  = str(size[0])\n",
    "    ET.SubElement(size_el, \"height\").text = str(size[1])\n",
    "    ET.SubElement(size_el, \"depth\").text  = \"3\"\n",
    "    for box, lbl in zip(boxes, labels):\n",
    "        obj = ET.SubElement(ann, \"object\")\n",
    "        ET.SubElement(obj, \"name\").text = lbl\n",
    "        bb = ET.SubElement(obj, \"bndbox\")\n",
    "        ET.SubElement(bb, \"xmin\").text = str(box[0])\n",
    "        ET.SubElement(bb, \"ymin\").text = str(box[1])\n",
    "        ET.SubElement(bb, \"xmax\").text = str(box[2])\n",
    "        ET.SubElement(bb, \"ymax\").text = str(box[3])\n",
    "    tree = ET.ElementTree(ann)\n",
    "    tree.write(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff0bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Preprocess & Augment\n",
    "def preprocess_and_augment(src_dir, label, num_aug=3, size=(300,300)):\n",
    "    for imgfn in os.listdir(src_dir):\n",
    "        if not imgfn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        im = Image.open(os.path.join(src_dir, imgfn)).convert(\"RGB\")\n",
    "        im = im.resize(size)\n",
    "        base, ext = os.path.splitext(imgfn)\n",
    "\n",
    "        # Original\n",
    "        out_name = f\"{label}_{base}{ext}\"\n",
    "        im.save(os.path.join(IMG_OUT, out_name))\n",
    "        xml_name = out_name.replace(ext, \".xml\")\n",
    "        write_voc_xml(\n",
    "            fn     = out_name,\n",
    "            size   = size,\n",
    "            boxes  = [[0, 0, size[0], size[1]]],\n",
    "            labels = [label],\n",
    "            dest   = os.path.join(ANN_OUT, xml_name)\n",
    "        )\n",
    "\n",
    "        # Horizontal flips\n",
    "        for i in range(num_aug):\n",
    "            aug = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            aug_name = f\"{label}_{base}_flip{i}{ext}\"\n",
    "            aug.save(os.path.join(IMG_OUT, aug_name))\n",
    "            xml_name = aug_name.replace(ext, \".xml\")\n",
    "            write_voc_xml(\n",
    "                fn     = aug_name,\n",
    "                size   = size,\n",
    "                boxes  = [[0, 0, size[0], size[1]]],\n",
    "                labels = [label],\n",
    "                dest   = os.path.join(ANN_OUT, xml_name)\n",
    "            )\n",
    "\n",
    "# Run preprocessing\n",
    "preprocess_and_augment(DEF_DIR, \"defect\",     num_aug=3)\n",
    "preprocess_and_augment(NON_DIR, \"non-defect\", num_aug=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fa0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Dataset & DataLoader\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, tf=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.tf      = tf\n",
    "        self.files   = [f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.files[idx]\n",
    "        im = Image.open(os.path.join(self.img_dir, fn)).convert(\"RGB\")\n",
    "        if self.tf:\n",
    "            im = self.tf(im)\n",
    "\n",
    "        tree = ET.parse(os.path.join(self.ann_dir, fn.replace(\".jpg\",\".xml\")))\n",
    "        root = tree.getroot()\n",
    "        boxes, labels = [], []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            b = obj.find(\"bndbox\")\n",
    "            boxes.append([\n",
    "                int(b.find(\"xmin\").text),\n",
    "                int(b.find(\"ymin\").text),\n",
    "                int(b.find(\"xmax\").text),\n",
    "                int(b.find(\"ymax\").text),\n",
    "            ])\n",
    "            lbl = obj.find(\"name\").text\n",
    "            labels.append(0 if lbl==\"defect\" else 1)\n",
    "\n",
    "        return (\n",
    "            im.to(device),\n",
    "            {\n",
    "                \"boxes\":  torch.tensor(boxes, dtype=torch.float32, device=device),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64, device=device)\n",
    "            }\n",
    "        )\n",
    "\n",
    "tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset     = VOCDataset(IMG_OUT, ANN_OUT, tf)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3664ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\USER/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:21<00:00, 25.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 6) Model Initialization\n",
    "model = ssd300_vgg16(\n",
    "    weights=None,\n",
    "    weights_backbone=VGG16_Weights.DEFAULT,    # ‚Üê valid enum alias for IMAGENET1K_V1\n",
    "    num_classes=3\n",
    ")\n",
    "model.to(device)\n",
    "print(\"Model on device:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4995f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Optimizer & Scheduler\n",
    "optimizer    = optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) mAP@0.5 Utility\n",
    "def calc_map(preds, targs, thr=0.5):\n",
    "    p_b, p_s, p_l = preds\n",
    "    t_b, t_l      = targs\n",
    "    precisions = []\n",
    "    for pb, ps, pl, tb, tl in zip(p_b, p_s, p_l, t_b, t_l):\n",
    "        if pb.numel()==0 or tb.numel()==0: continue\n",
    "        ious = box_iou(pb, tb)\n",
    "        tp   = (ious.max(1)[0] > thr).sum().item()\n",
    "        fp   = pb.size(0) - tp\n",
    "        precisions.append(tp / (tp + fp + 1e-8))\n",
    "    return sum(precisions)/len(precisions) if precisions else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a72102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 860/860 [02:21<00:00,  6.10it/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calc_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m---> 37\u001b[0m mAP50    \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_map\u001b[49m((all_pb, all_ps, all_pl), (all_tb, all_tl)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚Üí Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mAP@0.5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmAP50\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calc_map' is not defined"
     ]
    }
   ],
   "source": [
    "# 9) Training Loop\n",
    "NUM_EPOCHS = 15\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_pb, all_ps, all_pl = [], [], []\n",
    "    all_tb, all_tl         = [], []\n",
    "\n",
    "    loop = tqdm(data_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for imgs, targets in loop:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        for t in targets:\n",
    "            t[\"boxes\"]  = t[\"boxes\"].to(device)\n",
    "            t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outs = model(imgs)\n",
    "            for out, gt in zip(outs, targets):\n",
    "                keep = out[\"scores\"] > 0.3\n",
    "                all_pb.append(   out[\"boxes\"][keep].cpu()   )\n",
    "                all_ps.append(  out[\"scores\"][keep].cpu()   )\n",
    "                all_pl.append( out[\"labels\"][keep].cpu()   )\n",
    "                all_tb.append(   gt[\"boxes\"].cpu()           )\n",
    "                all_tl.append(  gt[\"labels\"].cpu()          )\n",
    "        model.train()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    mAP50    = calc_map((all_pb, all_ps, all_pl), (all_tb, all_tl)) * 100\n",
    "    print(f\"‚Üí Epoch {epoch}: Loss={avg_loss:.4f}, mAP@0.5={mAP50:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Save Model\n",
    "save_path = os.path.join(os.getcwd(), \"ssd_cans.pth\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"Model weights saved to:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
